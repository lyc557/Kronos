今日体会：真正消耗精力的，并不是模型微调，而是数据准备！
模型效果不理想，往往不是因为网络结构不够强，而是数据管线出了问题：CSV 清洗慢、交易日历错位、复权缺失、窗口边界处理混乱，最终把 GPU“饿着”，把 CPU“榨干”。

解决思路：用 Qlib 做数据源与预处理，把繁琐的清洗与对齐下沉到基础设施。我们只关注配置与特征，剩下交给 Qlib 的高性能数据加载与标准化流程。

数据准备模块（基于项目中的 `finetune/config.py` 与 `finetune/qlib_data_preprocess.py`）：
1）配置标的与窗口
- 路径：`finetune/config.py`
- 关键参数：
```
self.qlib_data_path = "~/.qlib/qlib_data/cn_data"
self.instrument = ['SH600009']
self.lookback_window = 90
self.predict_window = 10
self.dataset_path = "./data/processed_datasets"
self.feature_list = ['open', 'high', 'low', 'close', 'vol', 'amt']
```
这一步明确“只训练上海机场”，并定义模型需要的历史窗口与输出路径。

2）初始化与加载（自动时间对齐 + 复权数据）
- 路径：`finetune/qlib_data_preprocess.py`
- 关键流程：
- `initialize_qlib()`：`qlib.init(provider_uri=self.config.qlib_data_path, region=REG_CN)` 初始化中国市场数据环境；
- `load_qlib_data()`：通过 `D.calendar()` 计算真实加载区间（自动向前回溯 `lookback_window`，向后扩展 `predict_window`），使用 `QlibDataLoader` 高速读取后复权数据；随后 `pivot` 成行为时间、列为特征的矩阵，并生成衍生特征：`vol` 与 `amt`；过滤空值与不足长度的样本。

3）数据切分与落盘
- `prepare_dataset()`：按 `train/val/test` 时间切片，保存到 `dataset_path`：
- `train_data.pkl`、`val_data.pkl`、`test_data.pkl`
这些文件被下游的 `finetune/dataset.py` 直接读取并构造滑窗样本，训练过程不再被 CSV I/O 拖累。

4）一键生成数据集
运行：
```
python /home/luyangcai/code/Kronos/finetune/qlib_data_preprocess.py
```
执行完成后即可开始训练/验证流程，数据侧的瓶颈基本消除。

结论：当我们把数据准备从“手工 CSV”迁移到 Qlib 的数据层，训练的稳定性与效率会显著提升。真正消耗精力的，并不是模型结构，而是数据准备——而 Qlib 恰好让这件事变得“工程化、可复用、可维护”。